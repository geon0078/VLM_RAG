# pipeline.py

import torch
import gradio as gr

def run_vlm_only_pipeline(
    image, 
    user_question, 
    vlm_model, 
    text_tokenizer,
    vis_tokenizer
):
    """RAG ì—†ì´ VLM ë‹¨ë…ìœ¼ë¡œ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•©ë‹ˆë‹¤."""
    prompt_text = f"{user_question}\n<image>"
    
    prompt, input_ids, pixel_values = vlm_model.preprocess_inputs(prompt_text, [image], max_partition=9)
    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).unsqueeze(0).to(vlm_model.device)
    input_ids = input_ids.unsqueeze(0).to(vlm_model.device)
    pixel_values = [pixel_values.to(dtype=vis_tokenizer.dtype, device=vis_tokenizer.device)] if pixel_values is not None else None

    with torch.inference_mode():
        output_ids = vlm_model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, max_new_tokens=512, do_sample=False, eos_token_id=[text_tokenizer.eos_token_id, text_tokenizer.convert_tokens_to_ids("<|im_end|>")])[0]
        final_answer = text_tokenizer.decode(output_ids, skip_special_tokens=True).strip()
    
    return final_answer


# pipeline.py ì˜ run_vlm_only_pipeline í•¨ìˆ˜ ë¶€ë¶„

def run_vlm_only_pipeline(
    image, 
    user_question, 
    vlm_model, 
    text_tokenizer,
    vis_tokenizer
):
    """
    RAG ì—†ì´ VLM ë‹¨ë…ìœ¼ë¡œ ë‹µë³€í•˜ë©°, [ê·¼ê±°]ì™€ [ìµœì¢… ë‹µë³€]ì„ ë¶„ë¦¬í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # [ìˆ˜ì •] Chain-of-Thought í”„ë¡¬í”„íŠ¸ ì ìš©
    prompt_text = (
        "ë‹¹ì‹ ì€ ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ì§ˆë¬¸]ê³¼ [ì´ë¯¸ì§€]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ [í¬ë§·]ì— ë§ì¶° ë‹µë³€í•˜ì„¸ìš”.\n\n"
        "## [í¬ë§·]\n"
        "1. **[ê·¼ê±°]**: ì´ë¯¸ì§€ë¥¼ ê´€ì°°í•˜ê³  ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì—ì„œ ë°œê²¬í•œ êµ¬ì²´ì ì¸ ì‹œê°ì  ë‹¨ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.\n"
        "2. **[ìµœì¢… ë‹µë³€]**: ìœ„ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ìµœì¢… ê²°ë¡ ì„ ë‚´ë¦½ë‹ˆë‹¤.\n\n"
        "--- ì•„ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” ---\n"
        f"[ì§ˆë¬¸]: {user_question}\n"
        "<image>"
    )
    
    prompt, input_ids, pixel_values = vlm_model.preprocess_inputs(prompt_text, [image], max_partition=9)
    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).unsqueeze(0).to(vlm_model.device)
    input_ids = input_ids.unsqueeze(0).to(vlm_model.device)
    pixel_values = [pixel_values.to(dtype=vis_tokenizer.dtype, device=vis_tokenizer.device)] if pixel_values is not None else None

    with torch.inference_mode():
        output_ids = vlm_model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, max_new_tokens=1024, do_sample=False, eos_token_id=[text_tokenizer.eos_token_id, text_tokenizer.convert_tokens_to_ids("<|im_end|>")])[0]
        full_answer = text_tokenizer.decode(output_ids, skip_special_tokens=True).strip()
    
    # [ìˆ˜ì •] ëª¨ë¸ì˜ ë‹µë³€ì—ì„œ [ê·¼ê±°]ì™€ [ìµœì¢… ë‹µë³€]ì„ íŒŒì‹±í•˜ì—¬ ë¶„ë¦¬
    try:
        reasoning = full_answer.split("[ìµœì¢… ë‹µë³€]")[0].replace("[ê·¼ê±°]", "").strip()
        final_answer = full_answer.split("[ìµœì¢… ë‹µë³€]")[1].strip()
    except IndexError:
        # ëª¨ë¸ì´ í¬ë§·ì„ ë”°ë¥´ì§€ ì•Šì€ ê²½ìš°, ì „ì²´ ë‹µë³€ì„ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬
        reasoning = "ëª¨ë¸ì´ ë‹µë³€ í¬ë§·ì„ ë”°ë¥´ì§€ ì•Šì•„ ê·¼ê±°ë¥¼ ë¶„ë¦¬í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
        final_answer = full_answer
    
    return final_answer, reasoning

def run_rag_pipeline(
    image, 
    user_question, 
    vlm_model, 
    text_tokenizer, 
    vis_tokenizer, 
    embedding_model, 
    chroma_collection,
    progress=gr.Progress(track_tqdm=True)
):
    """ê¸°ì¡´ì˜ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    
    if chroma_collection is None:
        raise gr.Error("ChromaDB ì»¬ë ‰ì…˜ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")


    progress(0, desc="[RAG] ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘...")
    image_prompt = (
        "ì´ ì´ë¯¸ì§€ëŠ” ëŒ€í•œë¯¼êµ­ ì§€í•˜ì² ê³¼ ê´€ë ¨ëœ ì‚¬ì§„ì…ë‹ˆë‹¤. "
        "ë…¸ì„ , ì—­ëª…, í™˜ìŠ¹, ìƒ‰ìƒ, ì£¼ë³€ ì •ë³´ ë“± ì§€í•˜ì² ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ëœ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ, "
        "ì´ë¯¸ì§€ì—ì„œ íŒŒì•…í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ì •ë³´ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ë¬˜ì‚¬ë‚˜ ì¼ë°˜ì ì¸ ì„¤ëª…ì€ ìƒëµí•˜ê³ , "
        "ì§€í•˜ì²  ì‹œìŠ¤í…œê³¼ ê´€ë ¨ëœ ì‚¬ì‹¤ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”. <image>"
    )
    prompt, input_ids, pixel_values = vlm_model.preprocess_inputs(image_prompt, [image], max_partition=9)
    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).unsqueeze(0).to(vlm_model.device)
    input_ids = input_ids.unsqueeze(0).to(vlm_model.device)
    pixel_values_desc = [pixel_values.to(dtype=vis_tokenizer.dtype, device=vis_tokenizer.device)] if pixel_values is not None else None

    with torch.inference_mode():
        output_ids = vlm_model.generate(input_ids, pixel_values=pixel_values_desc, attention_mask=attention_mask, max_new_tokens=256, do_sample=False)[0]
        image_description = text_tokenizer.decode(output_ids, skip_special_tokens=True).strip()

    progress(0.2, desc="[RAG] ì´ë¯¸ì§€ ì„¤ëª… ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ ì¤‘...")
    image_query_embedding = embedding_model.encode([image_description]).tolist()
    image_results = chroma_collection.query(query_embeddings=image_query_embedding, n_results=3)
    image_retrieved_docs = image_results['documents'][0]
    image_context_str = "\n".join(image_retrieved_docs)

    progress(0.4, desc="[RAG] ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ ì¤‘...")
    question_query_embedding = embedding_model.encode([user_question]).tolist()
    question_results = chroma_collection.query(query_embeddings=question_query_embedding, n_results=3)
    question_retrieved_docs = question_results['documents'][0]
    question_context_str = "\n".join(question_retrieved_docs)

    # ë‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ë‘ ì°¸ê³ ì •ë³´ë¡œ ë„˜ê¹€
    context_str = (
        "[ì´ë¯¸ì§€ ì„¤ëª… ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼]\n" + (image_context_str if image_context_str.strip() else "ì—†ìŒ") +
        "\n\n[ì§ˆë¬¸ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼]\n" + (question_context_str if question_context_str.strip() else "ì—†ìŒ")
    )

    progress(0.6, desc="[RAG] ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
    final_prompt_text = (
        "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ì² ë„ ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [ì§€ì¹¨]ê³¼ [ì°¸ê³  ì •ë³´]ë¥¼ í™œìš©í•˜ì—¬ [ë¬¸ë§¥], [ì´ë¯¸ì§€] ì •ë³´ë¡œ [ì§ˆë¬¸]ì— ë‹µë³€í•˜ì„¸ìš”.\n\n"
        "## ì§€ì¹¨\n"
        "1. **ì¢…í•©ì  ë¶„ì„**: [ë¬¸ë§¥]ì— ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ë©´, ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì¦ê±°ë¥¼ ì°¾ìœ¼ì„¸ìš”.\n"
        "2. **í™˜ìŠ¹ì—­ ì²˜ë¦¬**: íŠ¹ì • ì—­ ì´ë¦„ì´ ì—¬ëŸ¬ ë…¸ì„  ì •ë³´ì™€ í•¨ê»˜ ë°˜ë³µë˜ë©´ 'í™˜ìŠ¹ì—­'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì´ ê²½ìš°, ê´€ë ¨ëœ ëª¨ë“  ë…¸ì„  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
        "3. **ì—„ê²©í•œ ê·¼ê±°**: ë°˜ë“œì‹œ [ë¬¸ë§¥], [ì´ë¯¸ì§€], [ì°¸ê³  ì •ë³´]ì—ì„œ ì°¾ì€ ëª…í™•í•œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì¶”ì¸¡í•˜ê±°ë‚˜ ì—†ëŠ” ì •ë³´ë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.\n"
        "4. **ì •ë³´ ë¶€ì¬ ì‹œ**: ë¶„ì„ ê²°ê³¼, ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ë°˜ë“œì‹œ \"ì§€ì‹ë² ì´ìŠ¤(VectorDB)ì™€ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì˜€ìœ¼ë‚˜ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        "## ì°¸ê³  ì •ë³´: ì„œìš¸ ì§€í•˜ì²  ë…¸ì„  ìƒ‰ìƒ ğŸ¨\n"
        "- 1í˜¸ì„ : ë‚¨ìƒ‰, 2í˜¸ì„ : ì´ˆë¡ìƒ‰, 3í˜¸ì„ : ì£¼í™©ìƒ‰, 4í˜¸ì„ : í•˜ëŠ˜ìƒ‰, 5í˜¸ì„ : ë³´ë¼ìƒ‰, 6í˜¸ì„ : í™©í† ìƒ‰, 7í˜¸ì„ : ì˜¬ë¦¬ë¸Œìƒ‰, 8í˜¸ì„ : ë¶„í™ìƒ‰, 9í˜¸ì„ : ê¸ˆìƒ‰\n\n"
        "--- ì•„ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” ---\n"
        f"[ë¬¸ë§¥]\n{context_str if context_str.strip() else 'ì—†ìŒ'}\n\n"
        f"[ì§ˆë¬¸]\n{user_question}\n<image>"
    )

    prompt, input_ids, pixel_values_final = vlm_model.preprocess_inputs(final_prompt_text, [image], max_partition=9)
    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).unsqueeze(0).to(vlm_model.device)
    input_ids = input_ids.unsqueeze(0).to(vlm_model.device)
    pixel_values_final = [pixel_values_final.to(dtype=vis_tokenizer.dtype, device=vis_tokenizer.device)] if pixel_values_final is not None else None

    with torch.inference_mode():
        output_ids = vlm_model.generate(input_ids, pixel_values=pixel_values_final, attention_mask=attention_mask, max_new_tokens=512, do_sample=False, eos_token_id=[text_tokenizer.eos_token_id, text_tokenizer.convert_tokens_to_ids("<|im_end|>")])[0]
        final_answer = text_tokenizer.decode(output_ids, skip_special_tokens=True).strip()

    return final_answer, image_description, context_str