# models.py

import torch
import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM
from config import VLM_MODEL_PATH, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, COLLECTION_NAME

def load_all_models():
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ì— í•„ìš”í•œ ëª¨ë“  ëª¨ë¸ê³¼ DB ì»¤ë„¥ì…˜ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print("Loading VLM model...")
    vlm_model = AutoModelForCausalLM.from_pretrained(
        VLM_MODEL_PATH,
        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        trust_remote_code=True,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    text_tokenizer = vlm_model.get_text_tokenizer()
    vis_tokenizer = vlm_model.get_visual_tokenizer()
    print("âœ… VLM model loaded.")

    print("Loading embedding model and ChromaDB...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)
    try:
        chroma_collection = client.get_collection(name=COLLECTION_NAME)
        print(f"âœ… ChromaDB collection '{COLLECTION_NAME}' loaded with {chroma_collection.count()} documents.")
    except Exception as e:
        print(f"ğŸš¨ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
        chroma_collection = None
    print("âœ… Embedding model and ChromaDB loaded.")

    return vlm_model, text_tokenizer, vis_tokenizer, embedding_model, chroma_collection