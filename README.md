# VLM-RAG: 시각-언어 모델을 위한 RAG 아키텍처 비교 프로젝트

이 프로젝트는 이미지와 텍스트를 함께 이해하는 시각-언어 모델(VLM)에 검색 증강 생성(RAG) 기술을 적용하는 두 가지 다른 아키텍처(V1, V3)를 구현하고 비교 분석합니다. 사용자는 Gradio로 제작된 웹 UI를 통해 두 모델의 성능 차이를 직관적으로 확인할 수 있습니다.

## 🚀 시작하기: Gradio 웹 UI 실행 방법 (권장)

이 프로젝트를 가장 쉽게 체험하는 방법은 Gradio 웹 UI를 실행하는 것입니다. 웹 UI를 통해 이미지를 업로드하고 질문을 입력하면, V1과 V3 모델의 답변을 나란히 비교하며 실시간 디버깅 로그까지 확인할 수 있습니다.

### 1. 사전 준비

- Python 3.10 이상
- Git

### 2. 설치

먼저, 프로젝트를 클론하고 필요한 라이브러리를 설치합니다.

```bash
# 1. 프로젝트 저장소를 클론합니다.
git clone https://github.com/geon0078/VLM_RAG.git
cd VLM_RAG

# 2. 가상 환경을 생성하고 활성화합니다. (권장)
python -m venv venv
source venv/bin/activate

# 3. requirements.txt 파일로 모든 종속성을 설치합니다.
pip install -r requirements.txt
```

### 3. Gradio 앱 실행

아래 명령어를 실행하여 웹 서버를 시작합니다.

```bash
python app.py
```

- **초기 모델 로딩:** 서버를 시작하면, 터미널에 V1과 V3 모델을 메모리로 로드하는 과정이 표시됩니다. 이 과정은 시스템 사양에 따라 **수 분이 소요될 수 있으며, 이는 정상적인 과정**입니다.
- **실행 확인:** 모델 로딩이 완료되면, 다음과 같은 URL이 터미널에 나타납니다.
  ```
  Running on local URL:  http://127.0.0.1:7860
  Running on public URL: https://....gradio.live
  ```
- **접속:** 웹 브라우저에서 `http://127.0.0.1:7860` 주소로 접속하여 웹 UI를 사용하세요. 외부에서 접속하려면 `https://....gradio.live` 주소를 사용하면 됩니다. (공유 URL은 72시간 동안 유효)

---

## 🏛️ 아키텍처 상세 설명

이 프로젝트는 두 가지 핵심 아키텍처를 구현하여 비교합니다.

### 1. V1 아키텍처: 이미지 중심의 RAG

![V1 Architecture Diagram](architecture_v1.md)

-   **개요:** V1은 이미지를 텍스트로 변환한 후, 그 텍스트 설명만을 단서로 데이터베이스를 검색하는 간단하고 직선적인 RAG 파이프라인입니다.
-   **작동 방식:**
    1.  **이미지 설명 생성:** VLM(Ovis)이 이미지를 보고 "지하철역 플랫폼 사진"과 같은 설명을 생성합니다.
    2.  **문서 검색:** 생성된 **이미지 설명**만을 이용해 VectorDB(ChromaDB)에서 관련 문서를 검색합니다. **(이때 사용자의 질문은 사용되지 않습니다.)**
    3.  **답변 생성:** 원본 이미지, 사용자 질문, 검색된 문서를 모두 VLM에 제공하여 최종 답변을 만듭니다.
-   **장점:**
    -   구조가 단순하여 이해하기 쉽습니다.
-   **단점:**
    -   **질문의 의도를 놓침:** 검색 시 질문을 사용하지 않으므로, 질문의 핵심(예: "주소가 어디야?")과 관련된 정보를 찾지 못할 가능성이 큽니다.
    -   **낮은 유연성:** 이미지에 없는 내용을 질문하면 답변 성능이 급격히 저하됩니다.

### 2. V3 아키텍처: 하이브리드 RAG

![V2 Architecture Diagram](architecture_v2.md)
*(참고: V3 디렉토리에 구현된 아키텍처입니다.)*

-   **개요:** V3는 V1의 단점을 극복하기 위해 두 개의 다른 AI 모델(VLM, LLM)이 협력하는 '하이브리드' 방식의 정교한 RAG 파이프라인입니다.
-   **작동 방식:**
    1.  **병렬 분석:**
        -   **VLM(Ovis):** 이미지를 분석하여 상세한 설명을 생성합니다.
        -   **LLM(Qwen):** 사용자의 질문을 분석하여 "역 이름", "다음 역" 같은 핵심 키워드를 추출합니다.
    2.  **쿼리 확장 및 검색:** **(1) 원본 질문, (2) 이미지 설명, (3) 핵심 키워드**를 모두 합쳐 '슈퍼 검색어'를 만들어 VectorDB를 검색합니다.
    3.  **답변 생성:** 원본 이미지, 검색된 문서, 이미지 설명 등을 모두 VLM에 제공하여 신뢰도 높은 최종 답변을 만듭니다.
-   **장점:**
    -   **높은 검색 정확도:** 이미지와 질문의 의도를 모두 반영하여 검색하므로 매우 정확합니다.
    -   **전문성을 통한 성능 극대화:** 이미지 분석은 VLM, 언어 이해는 LLM이 전담하여 성능을 높입니다.
    -   **뛰어난 유연성:** 질문의 의도를 파악하므로 이미지에 없는 정보도 효과적으로 검색하고 답변할 수 있습니다.
-   **단점:**
    -   V1에 비해 구조가 복잡합니다.

---

## 💻 기타 스크립트 실행 방법

### 개별 모델 CLI 실행

각 버전의 모델을 개별적으로 테스트할 수 있습니다.

```bash
# V1 모델 실행
python v1/main.py --image_path ./images/seoul_station.jpg --question "이 역의 이름은 무엇인가요?"

# V3 모델 실행
python v3/main.py --image_path ./images/seoul_station.jpg --question "이 역의 이름은 무엇인가요?"
```

### 전체 모델 비교 스크립트 실행

`gradio/questions.csv` 파일에 정의된 모든 질문-이미지 쌍에 대해 V1과 V3의 성능을 일괄적으로 비교하고, 결과를 `comparison_v1_v3.csv` 파일에 저장합니다.

```bash
python compare_models.py
```