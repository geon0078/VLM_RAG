import gradio as gr
import os
import sys
import torch
from PIL import Image
import time
import io
import contextlib

# --- í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì • ---
base_dir = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG'
sys.path.insert(0, base_dir)

# --- V1, V3 ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ ëª…í™•í•œ ê²½ë¡œë¡œ ì§ì ‘ ì„í¬íŠ¸ ---
try:
    from v1.module.models import load_all_models as load_v1_models
    from v1.module.pipeline import run_rag_pipeline as run_v1_pipeline

    from v3.module.models import load_models as load_v3_models
    from v3.module.retrieval import retrieve_context as retrieve_v3_context
    from v3.module.generation import generate_final_answer as generate_v3_answer
except ImportError as e:
    print(f"ëª¨ë“ˆ ì„í¬íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("VLM_RAG ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. v1ê³¼ v3 í´ë”ê°€ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.")
    sys.exit(1)

# --- ëª¨ë¸ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ ---
V1_MODELS = None
V3_MODELS = None

def load_models_globally():
    """
    Gradio ì•± ì‹œì‘ ì‹œ ëª¨ë“  ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    global V1_MODELS, V3_MODELS
    
    print("="*50)
    print("Gradio ì•± ì‹œì‘... ëª¨ë“  ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
    print("ì´ ê³¼ì •ì€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
    print("="*50)

    # stdoutì„ ë¦¬ë””ë ‰ì…˜í•˜ì—¬ ëª¨ë¸ ë¡œë”© ë¡œê·¸ë¥¼ UIì— í‘œì‹œí•  ìˆ˜ë„ ìˆì§€ë§Œ,
    # ë³µì¡ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ í„°ë¯¸ë„ì—ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.
    print("\n[1/2] V1 ëª¨ë¸ ë¡œë”© ì¤‘...")
    vlm_model_v1, text_tokenizer_v1, vis_tokenizer_v1, embedding_model_v1, collections_v1 = load_v1_models()
    V1_MODELS = (vlm_model_v1, text_tokenizer_v1, vis_tokenizer_v1, embedding_model_v1, collections_v1)
    print("âœ… V1 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    print("\n[2/2] V3 ëª¨ë¸ ë¡œë”© ì¤‘...")
    V3_MODELS = load_v3_models()
    print("âœ… V3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    
    print("\nğŸ‰ ëª¨ë“  ëª¨ë¸ ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. Gradio ì•±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("="*50)

# --- Gradio ì•±ì„ ìœ„í•œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ ---

def compare_v1_and_v3_stream(image, question):
    """
    Gradio ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ìƒì„±ê¸° í•¨ìˆ˜.
    ëª¨ë¸ ì‹¤í–‰ ë‹¨ê³„ë³„ë¡œ ë¡œê·¸ë¥¼ ìº¡ì²˜í•˜ê³  UIì— ì‹¤ì‹œê°„ìœ¼ë¡œ yieldí•©ë‹ˆë‹¤.
    """
    if image is None or question is None or question.strip() == "":
        yield {
            v1_answer_output: "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
            v1_reasoning_output: "",
            v1_debug_output: "",
            v3_answer_output: "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
            v3_reasoning_output: "",
            v3_debug_output: "",
        }
        return

    # UI ì´ˆê¸°í™”
    yield {
        v1_answer_output: "V1 ëª¨ë¸ ì‹¤í–‰ ì¤‘...",
        v1_reasoning_output: "",
        v1_debug_output: "",
        v3_answer_output: "V3 ëª¨ë¸ ì‹¤í–‰ ì¤‘...",
        v3_reasoning_output: "",
        v3_debug_output: "",
    }

    v1_log_stream = io.StringIO()
    v3_log_stream = io.StringIO()
    
    # --- V1 ì¶”ë¡  ì‹¤í–‰ ë° ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥ ---
    v1_answer, v1_reasoning = "", ""
    try:
        with contextlib.redirect_stdout(v1_log_stream):
            vlm_model_v1, text_tokenizer_v1, vis_tokenizer_v1, embedding_model_v1, collections_v1 = V1_MODELS
            v1_answer, v1_reasoning = run_v1_pipeline(
                image=image,
                user_question=question,
                vlm_model=vlm_model_v1,
                text_tokenizer=text_tokenizer_v1,
                vis_tokenizer=vis_tokenizer_v1,
                embedding_model=embedding_model_v1,
                chroma_collections=collections_v1
            )
        v1_debug_log = v1_log_stream.getvalue()
        yield { v1_debug_output: v1_debug_log, v1_answer_output: "V1 ëª¨ë¸ ì™„ë£Œ" }
    except Exception as e:
        v1_debug_log = f"V1 ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n{v1_log_stream.getvalue()}\n\n{e}"
        yield { v1_debug_output: v1_debug_log, v1_answer_output: "ì˜¤ë¥˜ ë°œìƒ" }


    # --- V3 ì¶”ë¡  ì‹¤í–‰ ë° ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥ ---
    v3_answer, v3_reasoning = "", ""
    try:
        with contextlib.redirect_stdout(v3_log_stream):
            retrieval_results = retrieve_v3_context(
                image=image,
                user_question=question,
                models=V3_MODELS
            )
            # ì¤‘ê°„ ë¡œê·¸ë¥¼ UIì— ì—…ë°ì´íŠ¸
            yield { v3_debug_output: v3_log_stream.getvalue() }

            v3_answer, v3_reasoning = generate_v3_answer(
                image=image,
                user_question=question,
                image_description=retrieval_results["image_description"],
                context=retrieval_results["retrieved_context"],
                vlm_model=V3_MODELS["vlm_model"],
                text_tokenizer=V3_MODELS["vlm_text_tokenizer"],
                vis_tokenizer=V3_MODELS["vlm_vis_tokenizer"]
            )
        v3_debug_log = v3_log_stream.getvalue()
        yield { v3_debug_output: v3_debug_log, v3_answer_output: "V3 ëª¨ë¸ ì™„ë£Œ" }
    except Exception as e:
        v3_debug_log = f"V3 ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n{v3_log_stream.getvalue()}\n\n{e}"
        yield { v3_debug_output: v3_debug_log, v3_answer_output: "ì˜¤ë¥˜ ë°œìƒ" }

    # --- ìµœì¢… ê²°ê³¼ UIì— í‘œì‹œ ---
    yield {
        v1_answer_output: v1_answer,
        v1_reasoning_output: v1_reasoning,
        v1_debug_output: v1_debug_log,
        v3_answer_output: v3_answer,
        v3_reasoning_output: v3_reasoning,
        v3_debug_output: v3_debug_log,
    }


# --- Gradio UI ì¸í„°í˜ì´ìŠ¤ ì •ì˜ ---

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš‡ V1 vs V3 ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¹„êµ (ì‹¤ì‹œê°„ ë¡œê·¸)")
    gr.Markdown("ëª¨ë¸ì´ ëª¨ë‘ ë¡œë“œëœ í›„ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, V1ê³¼ V3 ëª¨ë¸ì˜ ì‹¤í–‰ ë¡œê·¸ì™€ ìµœì¢… ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    with gr.Row():
        with gr.Column(scale=1):
            image_input = gr.Image(type="pil", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ")
            question_input = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆ: ì´ ì—­ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?")
            submit_btn = gr.Button("ê²°ê³¼ ë¹„êµí•˜ê¸°", variant="primary")
            
            image_dir = os.path.join(base_dir, "images")
            try:
                all_files = os.listdir(image_dir)
                image_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp']
                example_images = [os.path.join(image_dir, f) for f in all_files if os.path.splitext(f)[1].lower() in image_extensions]
                
                if example_images:
                    gr.Examples(
                        examples=example_images,
                        inputs=image_input,
                        label="ì˜ˆì‹œ ì´ë¯¸ì§€"
                    )
            except FileNotFoundError:
                gr.Markdown(f"âš ï¸ ì˜ˆì‹œ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: `{image_dir}`")

        with gr.Column(scale=2):
            with gr.Row():
                with gr.Column():
                    gr.Markdown("## V1 ëª¨ë¸ ê²°ê³¼")
                    v1_answer_output = gr.Textbox(label="V1 ìµœì¢… ë‹µë³€", lines=5, interactive=False)
                    v1_reasoning_output = gr.Textbox(label="V1 ê·¼ê±° ë° ì°¸ê³  ë¬¸ì„œ", lines=10, interactive=False)
                    with gr.Accordion("V1 ë””ë²„ê·¸ ë¡œê·¸ ë³´ê¸°", open=True):
                        v1_debug_output = gr.Textbox(label="V1 ì‹¤ì‹œê°„ ì¶œë ¥ ë¡œê·¸", lines=15, interactive=False)

                with gr.Column():
                    gr.Markdown("## V3 ëª¨ë¸ ê²°ê³¼")
                    v3_answer_output = gr.Textbox(label="V3 ìµœì¢… ë‹µë³€", lines=5, interactive=False)
                    v3_reasoning_output = gr.Textbox(label="V3 ê·¼ê±° ë° ì°¸ê³  ë¬¸ì„œ (ì ìˆ˜ í¬í•¨)", lines=10, interactive=False)
                    with gr.Accordion("V3 ë””ë²„ê·¸ ë¡œê·¸ ë³´ê¸°", open=True):
                        v3_debug_output = gr.Textbox(label="V3 ì‹¤ì‹œê°„ ì¶œë ¥ ë¡œê·¸", lines=15, interactive=False)

    # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜ì— ì—°ê²°
    submit_btn.click(
        fn=compare_v1_and_v3_stream,
        inputs=[image_input, question_input],
        outputs=[v1_answer_output, v1_reasoning_output, v1_debug_output, v3_answer_output, v3_reasoning_output, v3_debug_output]
    )

if __name__ == "__main__":
    # 1. ì•± ì‹œì‘ ì‹œ ëª¨ë¸ì„ ì „ì—­ì ìœ¼ë¡œ ë¡œë“œ
    load_models_globally()
    
    # 2. ëª¨ë¸ ë¡œë“œê°€ ì™„ë£Œëœ í›„ Gradio ì•± ì‹¤í–‰
    demo.launch(share=True)
