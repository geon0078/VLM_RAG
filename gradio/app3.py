# app2.py

import os
import logging
from typing import Tuple, Optional, Any
from contextlib import contextmanager

# GPU ì„¤ì •ì„ ìµœìƒë‹¨ìœ¼ë¡œ ì´ë™
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # ë‘ ë²ˆì§¸ GPUë§Œ ì‚¬ìš©

import gradio as gr
from models import load_all_models
from pipeline import run_rag_pipeline, run_vlm_only_pipeline
from PIL import Image

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ModelManager:
    """ëª¨ë¸ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.models_loaded = False
        self.vlm = None
        self.txt_tokenizer = None
        self.vis_tokenizer = None
        self.emb_model = None
        self.collections = None
    
    def load_models(self) -> bool:
        """ëª¨ë“  ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤."""
        try:
            logger.info("ğŸš€ ë¹„êµ ì•± ì‹œì‘... ëª¨ë“  ëª¨ë¸ ë¡œë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            
            self.vlm, self.txt_tokenizer, self.vis_tokenizer, self.emb_model, self.collections = load_all_models()
            
            logger.info("âœ¨ ëª¨ë“  ëª¨ë¸ê³¼ DBê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. Gradio UIë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
            self.models_loaded = True
            return True
            
        except Exception as e:
            logger.error(f"ğŸš¨ ì¹˜ëª…ì  ì˜¤ë¥˜: ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. {e}")
            self.models_loaded = False
            return False
    
    def is_ready(self) -> bool:
        """ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return self.models_loaded

# ì „ì—­ ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
model_manager = ModelManager()

@contextmanager
def error_handler(operation_name: str):
    """ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    try:
        yield
    except Exception as e:
        logger.error(f"{operation_name} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise gr.Error(f"{operation_name} ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def validate_inputs(image: Optional[Image.Image], question: str) -> None:
    """ì…ë ¥ê°’ ê²€ì¦"""
    if image is None:
        raise gr.Error("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    if not question or question.strip() == "":
        raise gr.Error("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    if len(question.strip()) < 3:
        raise gr.Error("ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 3ê¸€ì ì´ìƒ ì…ë ¥í•´ì£¼ì„¸ìš”.")

def gradio_comparison_interface(
    image: Optional[Image.Image], 
    user_question: str, 
    progress=gr.Progress(track_tqdm=True)
) -> Tuple[str, str, str, str, str]:
    """VLM ë‹¨ë… ë‹µë³€ê³¼ RAG ì ìš© ë‹µë³€ì„ ëª¨ë‘ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # ëª¨ë¸ ìƒíƒœ í™•ì¸
    if not model_manager.is_ready():
        raise gr.Error("ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ê³  ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ì…ë ¥ê°’ ê²€ì¦
    validate_inputs(image, user_question)
    
    # ì§ˆë¬¸ ì „ì²˜ë¦¬
    user_question = user_question.strip()
    
    try:
        # VLM ë‹¨ë… ë‹µë³€ ìƒì„±
        with error_handler("VLM ë‹¨ë… ë‹µë³€ ìƒì„±"):
            progress(0.1, desc="VLM ë‹¨ë… ë‹µë³€ ìƒì„± ì¤‘...")
            vlm_answer, vlm_reasoning = run_vlm_only_pipeline(
                image, 
                user_question, 
                model_manager.vlm, 
                model_manager.txt_tokenizer, 
                model_manager.vis_tokenizer
            )
        
        # RAG ì ìš© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        with error_handler("RAG ì ìš© ë‹µë³€ ìƒì„±"):
            progress(0.5, desc="RAG ì ìš© ë‹µë³€ ìƒì„± ì¤‘...")
            rag_answer, rag_desc, rag_context = run_rag_pipeline(
                image, 
                user_question, 
                model_manager.vlm, 
                model_manager.txt_tokenizer, 
                model_manager.vis_tokenizer, 
                model_manager.emb_model, 
                model_manager.collections, 
                progress
            )
        
        progress(1.0, desc="ì™„ë£Œ!")
        
        # ê²°ê³¼ ê²€ì¦
        if not vlm_answer or not rag_answer:
            raise gr.Error("ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        return vlm_answer, vlm_reasoning, rag_answer, rag_desc, rag_context
        
    except gr.Error:
        # Gradio ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise gr.Error(f"ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

def create_textbox_config(label: str, lines: int = 8, scrollable: bool = True) -> dict:
    """í…ìŠ¤íŠ¸ë°•ìŠ¤ ê³µí†µ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    config = {
        "label": label,
        "interactive": False,
        "show_copy_button": True,
        "show_label": True,
    }
    
    if scrollable:
        # ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë°•ìŠ¤ ì„¤ì •
        config.update({
            "lines": lines,
            "max_lines": lines,  # linesì™€ ë™ì¼í•˜ê²Œ ì„¤ì •í•˜ì—¬ ê³ ì • ë†’ì´
            "autoscroll": False  # ìë™ ìŠ¤í¬ë¡¤ ë¹„í™œì„±í™”ë¡œ ì‚¬ìš©ìê°€ ì§ì ‘ ì œì–´
        })
    else:
        # ìë™ í¬ê¸° ì¡°ì ˆ í…ìŠ¤íŠ¸ë°•ìŠ¤
        config.update({
            "lines": lines,
            "autoscroll": True
        })
    
    return config

def create_demo() -> gr.Blocks:
    """Gradio ë°ëª¨ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    with gr.Blocks(
        theme=gr.themes.Soft(), 
        title="VLM vs RAG-VLM",
        css="""
        .container { max-width: 1400px; margin: auto; }
        .gradio-container { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
        
        /* í…ìŠ¤íŠ¸ë°•ìŠ¤ ìŠ¤í¬ë¡¤ ê°•ì œ ì ìš© */
        .scroll-area textarea {
            overflow-y: auto !important;
            resize: vertical !important;
            max-height: 300px !important;
            height: 200px !important;
        }
        
        /* Gradio í…ìŠ¤íŠ¸ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ê°œì„  */
        .gr-textbox textarea {
            overflow-y: auto !important;
            resize: vertical !important;
        }
        
        /* ì•„ì½”ë””ì–¸ ë‚´ë¶€ í…ìŠ¤íŠ¸ë°•ìŠ¤ */
        .gr-accordion .gr-textbox textarea {
            max-height: 200px !important;
            height: 150px !important;
            overflow-y: auto !important;
        }
        """
    ) as demo:
        
        gr.Markdown("# ğŸ¤– VLM vs RAG-VLM ë¹„êµ ë¶„ì„")
        gr.Markdown("ë™ì¼í•œ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì— ëŒ€í•´ VLM ë‹¨ë… ë‹µë³€ê³¼ RAG ì ìš© ë‹µë³€ì„ ë¹„êµí•©ë‹ˆë‹¤.")
        
        if not model_manager.is_ready():
            gr.Markdown("## ğŸš¨ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨!")
            gr.Markdown("ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.")
            return demo
        
        gr.Markdown("---")

        with gr.Row():
            # ì…ë ¥ ì„¹ì…˜
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“¤ ì…ë ¥")
                image_input = gr.Image(
                    type="pil", 
                    label="ì´ë¯¸ì§€ ì—…ë¡œë“œ",
                    height=300
                )
                question_input = gr.Textbox(
                    label="ì§ˆë¬¸ ì…ë ¥", 
                    placeholder="ì´ë¯¸ì§€ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”...",
                    lines=3
                )
                
                with gr.Row():
                    submit_button = gr.Button(
                        "ğŸ” ê²°ê³¼ ë¹„êµí•˜ê¸°", 
                        variant="primary",
                        size="lg"
                    )
                    clear_button = gr.Button(
                        "ğŸ—‘ï¸ ì´ˆê¸°í™”", 
                        variant="secondary"
                    )
            
            # VLM ë‹¨ë… ë‹µë³€ ì„¹ì…˜
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ§  VLM ë‹¨ë… ë‹µë³€")
                vlm_answer_output = gr.Textbox(
                    **create_textbox_config("ëª¨ë¸ì˜ ìì²´ ì§€ì‹ ê¸°ë°˜ ë‹µë³€", lines=10, scrollable=True),
                    elem_classes=["scroll-area"]
                )
                
                with gr.Accordion("ğŸ’­ ë‹µë³€ ê·¼ê±° ë³´ê¸°", open=False):
                    vlm_reasoning_output = gr.Textbox(
                        **create_textbox_config("VLMì˜ ì¶”ë¡  ê³¼ì •", lines=8, scrollable=True),
                        elem_classes=["scroll-area"]
                    )

            # RAG ì ìš© ë‹µë³€ ì„¹ì…˜
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“š RAG ì ìš© ë‹µë³€")
                rag_answer_output = gr.Textbox(
                    **create_textbox_config("ì§€ì‹ë² ì´ìŠ¤(DB) ì°¸ê³  ë‹µë³€", lines=10, scrollable=True),
                    elem_classes=["scroll-area"]
                )
                
                with gr.Accordion("ğŸ” ìƒì„¸ ê³¼ì • ë³´ê¸°", open=False):
                    rag_desc_output = gr.Textbox(
                        **create_textbox_config("ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼", lines=8, scrollable=True),
                        elem_classes=["scroll-area"]
                    )
                    rag_context_output = gr.Textbox(
                        **create_textbox_config("ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼", lines=8, scrollable=True),
                        elem_classes=["scroll-area"]
                    )

        # ì´ë²¤íŠ¸ ë°”ì¸ë”©
        submit_button.click(
            fn=gradio_comparison_interface,
            inputs=[image_input, question_input],
            outputs=[
                vlm_answer_output, 
                vlm_reasoning_output, 
                rag_answer_output, 
                rag_desc_output, 
                rag_context_output
            ]
        )
        
        # ì´ˆê¸°í™” ë²„íŠ¼ ê¸°ëŠ¥
        clear_button.click(
            fn=lambda: (None, "", "", "", "", "", ""),
            outputs=[
                image_input, 
                question_input, 
                vlm_answer_output, 
                vlm_reasoning_output, 
                rag_answer_output, 
                rag_desc_output, 
                rag_context_output
            ]
        )
        
        # ì‚¬ìš© ê°€ì´ë“œ
        with gr.Accordion("ğŸ“– ì‚¬ìš© ê°€ì´ë“œ", open=False):
            gr.Markdown("""
            ### ì‚¬ìš© ë°©ë²•
            1. **ì´ë¯¸ì§€ ì—…ë¡œë“œ**: ë¶„ì„í•˜ê³ ì í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”
            2. **ì§ˆë¬¸ ì…ë ¥**: ì´ë¯¸ì§€ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”
            3. **ê²°ê³¼ ë¹„êµ**: ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ VLM ë‹¨ë… ë‹µë³€ê³¼ RAG ì ìš© ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”
            
            ### íŒ
            - êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë” ì •í™•í•œ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
            - ì´ë¯¸ì§€ì˜ í•´ìƒë„ê°€ ë†’ì„ìˆ˜ë¡ ë” ë‚˜ì€ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤
            - ê° ë‹µë³€ì˜ ê·¼ê±°ì™€ ê³¼ì •ë„ í•¨ê»˜ í™•ì¸í•´ë³´ì„¸ìš”
            """)
    
    return demo

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª¨ë¸ ë¡œë”©
    success = model_manager.load_models()
    
    if not success:
        logger.error("ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # Gradio ì•± ìƒì„± ë° ì‹¤í–‰
    demo = create_demo()
    
    try:
        demo.launch(
            server_name="0.0.0.0",  # ì™¸ë¶€ ì ‘ì† í—ˆìš©
            server_port=7860,       # í¬íŠ¸ ëª…ì‹œ
            share=False,            # ê³µê°œ ë§í¬ ìƒì„± ì•ˆí•¨
            debug=False,            # í”„ë¡œë•ì…˜ì—ì„œëŠ” False
            show_error=True,        # ì—ëŸ¬ í‘œì‹œ
            quiet=False             # ë¡œê·¸ í‘œì‹œ
        )
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()