
# /home/aisw/Project/UST-ETRI-2025/VLM_RAG/make_vector_db_/line_data_v2.py
import os
import pandas as pd
from sentence_transformers import SentenceTransformer
import torch
import chromadb
from tqdm import tqdm

# --- ì„¤ì •ë¶€ ---
DATA_FOLDER = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/csv'
EMBEDDING_MODEL = 'BAAI/bge-m3'
PERSIST_DIRECTORY = '/home/aisw/Project/UST-ETRI-2025/chroma_db'
# ìƒˆë¡œìš´ ì»¬ë ‰ì…˜ ì´ë¦„
NEW_COLLECTION_NAME = 'subway_line_info_v1'

def load_csv_files(folder_path):
    """ì§€ì •ëœ í´ë”ì—ì„œ .csv íŒŒì¼ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    csv_files = []
    for filename in os.listdir(folder_path):
        # 'convert'ê°€ í¬í•¨ëœ íŒŒì¼ì€ ë³€í™˜ìš© ìŠ¤í¬ë¦½íŠ¸ì´ë¯€ë¡œ ì œì™¸
        if filename.lower().endswith('.csv') and 'convert' not in filename:
            csv_files.append(os.path.join(folder_path, filename))
    return csv_files

def build_atomic_documents_from_csv(file_path):
    """
    CSV íŒŒì¼ì—ì„œ ì—­ ìˆœì„œ ì •ë³´ë¥¼ ì½ì–´,
    ì—­ê³¼ ì—­ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì›ìì  ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    line_name = os.path.splitext(os.path.basename(file_path))[0]
    df = pd.read_csv(file_path)
    # 'ì—­ëª…(í•œê¸€)' ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆì„ ê²½ìš° ì œê±°í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    station_names = df['ì—­ëª…(í•œê¸€)'].dropna().tolist()
    
    documents = []
    metadatas = []
    ids = []

    # ì—­ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° "ì´ì „-ë‹¤ìŒ" ê´€ê³„ì˜ ë¬¸ì„œë¥¼ ìƒì„±
    for i in range(len(station_names) - 1):
        prev_station = station_names[i]
        next_station = station_names[i+1]

        # ë‹¤ìŒ ì—­ ì •ë³´ ë¬¸ì„œ
        doc_next = f"{line_name}ì—ì„œ {prev_station}ì˜ ë‹¤ìŒ ì—­ì€ {next_station}ì…ë‹ˆë‹¤."
        meta_next = {'ë…¸ì„ ': line_name, 'ì—­': prev_station, 'ë°©í–¥': 'ë‹¤ìŒ'}
        id_next = f"{line_name}_{prev_station}_next"
        documents.append(doc_next)
        metadatas.append(meta_next)
        ids.append(id_next)

        # ì´ì „ ì—­ ì •ë³´ ë¬¸ì„œ
        doc_prev = f"{line_name}ì—ì„œ {next_station}ì˜ ì´ì „ ì—­ì€ {prev_station}ì…ë‹ˆë‹¤."
        meta_prev = {'ë…¸ì„ ': line_name, 'ì—­': next_station, 'ë°©í–¥': 'ì´ì „'}
        id_prev = f"{line_name}_{next_station}_prev"
        documents.append(doc_prev)
        metadatas.append(meta_prev)
        ids.append(id_prev)
        
    return documents, metadatas, ids

def main():
    print("ğŸ“‚ CSV íŒŒì¼ íƒìƒ‰ ì¤‘...")
    csv_files = load_csv_files(DATA_FOLDER)
    if not csv_files:
        print("âš ï¸ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    print(f"âœ… ì´ {len(csv_files)}ê°œ ë…¸ì„  CSV íŒŒì¼ ë°œê²¬.")
    
    all_documents = []
    all_metadatas = []
    all_ids = []

    for csv_file in tqdm(csv_files, desc="[Processing Lines]"):
        try:
            docs, metas, ids = build_atomic_documents_from_csv(csv_file)
            all_documents.extend(docs)
            all_metadatas.extend(metas)
            all_ids.extend(ids)
        except Exception as e:
            line_name = os.path.splitext(os.path.basename(csv_file))[0]
            print(f"ğŸš¨ '{line_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    print(f"âœ… ì´ {len(all_documents)}ê°œì˜ ì—­ ê´€ê³„ ë¬¸ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {EMBEDDING_MODEL}")
    model = SentenceTransformer(EMBEDDING_MODEL)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    print(f"â³ ì„ë² ë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {device})")
    embeddings = model.encode(all_documents, show_progress_bar=True, device=device)

    print("ğŸ’¾ ChromaDB ì €ì¥ ì‹œì‘...")
    client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆë‹¤ë©´ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±
    if NEW_COLLECTION_NAME in [c.name for c in client.list_collections()]:
        client.delete_collection(name=NEW_COLLECTION_NAME)
        print(f"ğŸ’¥ ê¸°ì¡´ '{NEW_COLLECTION_NAME}' ì»¬ë ‰ì…˜ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        
    collection = client.create_collection(
        name=NEW_COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"}
    )
    
    # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥
    batch_size = 5000
    for i in tqdm(range(0, len(all_documents), batch_size), desc="[Saving to DB]"):
        collection.add(
            embeddings=embeddings[i:i+batch_size].tolist(),
            documents=all_documents[i:i+batch_size],
            metadatas=all_metadatas[i:i+batch_size],
            ids=all_ids[i:i+batch_size]
        )

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {collection.count()}ê±´")
    print(f"(DB ìœ„ì¹˜: {os.path.abspath(PERSIST_DIRECTORY)})")

if __name__ == '__main__':
    main()
