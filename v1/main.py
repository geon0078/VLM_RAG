import torch
import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM
import argparse
from PIL import Image

# --- config.py ë‚´ìš© ---
EMBEDDING_MODEL_NAME = 'BAAI/bge-m3'
VLM_MODEL_PATH = "AIDC-AI/Ovis2-8B"
PERSIST_DIRECTORY = '/home/aisw/Project/UST-ETRI-2025/chroma_db'
COLLECTION_NAMES = [
    "korean_knowledge_base_v2",
    "subway_multilang_all_lines"
]

# --- models.py ë‚´ìš© ---
def load_all_models():
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ì— í•„ìš”í•œ ëª¨ë“  ëª¨ë¸ê³¼ DB ì»¤ë„¥ì…˜ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print("VLM ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
    vlm_model = AutoModelForCausalLM.from_pretrained(
        VLM_MODEL_PATH,
        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        trust_remote_code=True,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    text_tokenizer = vlm_model.get_text_tokenizer()
    vis_tokenizer = vlm_model.get_visual_tokenizer()
    print("âœ… VLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    print("ì„ë² ë”© ëª¨ë¸ê³¼ ChromaDBë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)
    
    collections = {}
    for cname in COLLECTION_NAMES:
        try:
            cobj = client.get_collection(name=cname)
            print(f"âœ… ChromaDB ì»¬ë ‰ì…˜ '{cname}' ë¡œë“œ ì™„ë£Œ ({cobj.count()}ê°œ ë¬¸ì„œ).")
            collections[cname] = cobj
        except Exception as e:
            print(f"ğŸš¨ '{cname}' ì»¬ë ‰ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            collections[cname] = None
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë° ChromaDB ë¡œë“œ ì™„ë£Œ.")

    return vlm_model, text_tokenizer, vis_tokenizer, embedding_model, collections

# --- pipeline.py ë‚´ìš© ---
def run_rag_pipeline_v2(
    image, 
    user_question, 
    vlm_model, 
    text_tokenizer, 
    vis_tokenizer, 
    embedding_model, 
    chroma_collections
):
    """
    ê°œì„ ëœ RAG íŒŒì´í”„ë¼ì¸ (RAG-Fusion ìŠ¤íƒ€ì¼)
    """
    # if not chroma_collections or not isinstance(chroma_collections, dict) or all(v is None for v in chroma_collections.values()):
    #     raise ValueError("ChromaDB ì»¬ë ‰ì…˜ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # 1. ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì„¤ëª… ìƒì„± (CoT í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
    print("[RAG] ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì¤‘...")
    image_prompt = (
        "ë‹¹ì‹ ì€ ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì´ë¯¸ì§€ë¥¼ ê´€ì°°í•˜ê³ , ì´ë¯¸ì§€ì— ë‚˜íƒ€ë‚œ ëª¨ë“  í…ìŠ¤íŠ¸, í‘œì§€íŒ, ë…¸ì„ ë„, ìƒ‰ìƒ, ìƒì§•ë¬¼ ë“± "
        "ëŒ€í•œë¯¼êµ­ ì§€í•˜ì²  ì‹œìŠ¤í…œê³¼ ê´€ë ¨ëœ ëª¨ë“  ì‹œê°ì  ì •ë³´ë¥¼ ìƒì„¸í•˜ê³  êµ¬ì¡°ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”. "
        "íŠ¹íˆ, ì—¬ëŸ¬ ì •ë³´ê°€ ë³µí•©ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš°(ì˜ˆ: í™˜ìŠ¹ì—­ í‘œì§€íŒ) ê´€ê³„ë¥¼ ëª…í™•íˆ í•˜ì—¬ ì„œìˆ í•˜ì„¸ìš”. <image>"
    )
    prompt, input_ids, pixel_values = vlm_model.preprocess_inputs(image_prompt, [image], max_partition=9)
    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).unsqueeze(0).to(vlm_model.device)
    input_ids = input_ids.unsqueeze(0).to(vlm_model.device)
    pixel_values_desc = [pixel_values.to(dtype=vis_tokenizer.dtype, device=vis_tokenizer.device)] if pixel_values is not None else None

    with torch.inference_mode():
        output_ids = vlm_model.generate(input_ids, pixel_values=pixel_values_desc, attention_mask=attention_mask, max_new_tokens=1024, do_sample=False)[0]
        image_description = text_tokenizer.decode(output_ids, skip_special_tokens=True).strip()
    print(f"âœ… ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì™„ë£Œ:\n{image_description}")

    # 2. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì´ë¯¸ì§€ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ë¹„í™œì„±í™”)
    # print("[RAG] ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì¤‘...")
    # query_generation_prompt = (
    #     f"ë‹¹ì‹ ì€ ë˜‘ë˜‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±ê¸°ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ê³¼ ì´ë¯¸ì§€ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ, VectorDBì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n\n"
    #     f"**[ì›ë³¸ ì§ˆë¬¸]**: {user_question}\n"
    #     f"**[ì´ë¯¸ì§€ ì„¤ëª…]**: {image_description}\n\n"
    #     f"**[ì§€ì¹¨]**:\n"
    #     f"- ì›ë³¸ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ, ë‹¤ë¥¸ ê´€ì ì—ì„œ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ì„¸ìš”.\n"
    #     f"- ì´ë¯¸ì§€ ì„¤ëª…ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œ(ì—­ ì´ë¦„, ë…¸ì„  ë²ˆí˜¸, ë°©í–¥ ë“±)ë¥¼ í™œìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì¿¼ë¦¬ë¥¼ ë§Œë“œì„¸ìš”.\n"
    #     f"- ìƒì„±ëœ ì¿¼ë¦¬ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ í˜•íƒœì™€ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ë„ë¡ ë‹¤ì–‘í™”í•˜ì„¸ìš”.\n"
    #     f"- ê° ì¿¼ë¦¬ëŠ” í•œ ì¤„ë¡œ ì‘ì„±í•˜ê³ , êµ¬ë¶„ì ì—†ì´ ë‚´ìš©ë§Œ ì œì‹œí•˜ì„¸ìš”."
    # )
    # 
    # prompt, input_ids, _ = vlm_model.preprocess_inputs(query_generation_prompt, [], max_partition=1)
    # attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).unsqueeze(0).to(vlm_model.device)
    # input_ids = input_ids.unsqueeze(0).to(vlm_model.device)

    # with torch.inference_mode():
    #     output_ids = vlm_model.generate(input_ids, pixel_values=[], attention_mask=attention_mask, max_new_tokens=256, do_sample=False)[0]
    #     generated_queries_text = text_tokenizer.decode(output_ids, skip_special_tokens=True).strip()
    
    queries = [user_question] #+ [q.strip() for q in generated_queries_text.split('\n') if q.strip()]
    print(f"âœ… ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬:\n{queries}")

    # 3. ìƒì„±ëœ ëª¨ë“  ì¿¼ë¦¬ë¡œ VectorDB ê²€ìƒ‰ ìˆ˜í–‰
    print(f"[RAG] {len(queries)}ê°œ ì¿¼ë¦¬ë¡œ ë²¡í„° ê²€ìƒ‰ ì¤‘...")
    all_retrieved_docs = {}
    for query in queries:
        query_embedding = embedding_model.encode([query]).tolist()
        for cname, collection in chroma_collections.items():
            if collection is not None:
                try:
                    results = collection.query(query_embeddings=query_embedding, n_results=3)
                    for doc_id, doc_content in zip(results.get('ids', [[]])[0], results.get('documents', [[]])[0]):
                        all_retrieved_docs[doc_id] = doc_content
                except Exception as e:
                    print(f"[RAG] ì¿¼ë¦¬ '{query}'ì— ëŒ€í•œ ì»¬ë ‰ì…˜ '{cname}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    context_str = "\n".join(all_retrieved_docs.values())
    print(f"âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ. {len(all_retrieved_docs)}ê°œì˜ ê³ ìœ  ë¬¸ì„œ ê²€ìƒ‰ë¨.")

    # 5. ìµœì¢… ë‹µë³€ ìƒì„±
    print("[RAG] ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
    final_prompt_text = f"""ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ì§€í•˜ì²  ì •ë³´ì— ëŠ¥í†µí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ [ì§€ì¹¨]ì— ë”°ë¼, ì£¼ì–´ì§„ [ì°¸ê³  ì •ë³´], [ì´ë¯¸ì§€ ì„¤ëª…], [ì´ë¯¸ì§€]ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ë‹µë³€í•˜ì„¸ìš”.

## [ì§€ì¹¨]
1. **ì •ë³´ ì¢…í•©**: [ì°¸ê³  ì •ë³´]ëŠ” ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ë‹¤ì–‘í•œ ë¬¸ì„œë“¤ì˜ ì§‘í•©ì…ë‹ˆë‹¤. ëª¨ë“  ë¬¸ì„œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¥ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.
2. **ê·¼ê±° ì œì‹œ**: ë‹µë³€ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´, ì–´ë–¤ [ì°¸ê³  ì •ë³´]ë‚˜ [ì´ë¯¸ì§€]ì˜ ì‹œê°ì  ë‹¨ì„œë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©í–ˆëŠ”ì§€ ëª…í™•íˆ ì–¸ê¸‰í•˜ì„¸ìš”. (ì˜ˆ: 'ì¶©ë¬´ë¡œì—­ ì •ë³´ì— ë”°ë¥´ë©´...', 'ì´ë¯¸ì§€ ì† 4í˜¸ì„  ì•ˆë‚´ í‘œì§€íŒì„ ë³´ë©´...')
3. **ì¶”ë¡  ê¸ˆì§€**: ì œê³µëœ ì •ë³´ ë‚´ì—ì„œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ì„ í™•ì‹ í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, \"ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. **ë‹µë³€ í˜•ì‹**: ìµœì¢… ë‹µë³€ì€ ì•„ë˜ [í¬ë§·]ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.

## [í¬ë§·]
**[ìµœì¢… ë‹µë³€]**
[ì—¬ê¸°ì— ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ì„œìˆ í•©ë‹ˆë‹¤.]

**[ê·¼ê±°]**
- [ì‚¬ìš©í•œ ê·¼ê±° 1]
- [ì‚¬ìš©í•œ ê·¼ê±° 2]
...

--- ì•„ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš” ---
**[ì§ˆë¬¸]**: {user_question}

**[ì´ë¯¸ì§€ ì„¤ëª…]**:
{image_description}

**[ì°¸ê³  ì •ë³´]**:
{context_str if context_str.strip() else 'ì—†ìŒ'}

<image>"""

    prompt, input_ids, pixel_values_final = vlm_model.preprocess_inputs(final_prompt_text, [image], max_partition=9)
    attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id).unsqueeze(0).to(vlm_model.device)
    input_ids = input_ids.unsqueeze(0).to(vlm_model.device)
    pixel_values_final = [pixel_values_final.to(dtype=vis_tokenizer.dtype, device=vis_tokenizer.device)] if pixel_values_final is not None else None

    with torch.inference_mode():
        output_ids = vlm_model.generate(input_ids, pixel_values=pixel_values_final, attention_mask=attention_mask, max_new_tokens=1024, do_sample=False, eos_token_id=[text_tokenizer.eos_token_id, text_tokenizer.convert_tokens_to_ids("<|im_end|>")])[0]
        final_answer_full = text_tokenizer.decode(output_ids, skip_special_tokens=True).strip()

    # ë‹µë³€ê³¼ ê·¼ê±° ë¶„ë¦¬
    try:
        final_answer = final_answer_full.split("**[ê·¼ê±°]**")[0].replace("**[ìµœì¢… ë‹µë³€]**", "").strip()
        reasoning = final_answer_full.split("**[ê·¼ê±°]**")[1].strip()
    except IndexError:
        final_answer = final_answer_full
        reasoning = "ê·¼ê±°ë¥¼ ë¶„ë¦¬í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

    return final_answer, reasoning, image_description, context_str

def main():
    parser = argparse.ArgumentParser(description="VLM RAG íŒŒì´í”„ë¼ì¸")
    parser.add_argument("--image_path", type=str, required=True, help="ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--question", type=str, required=True, help="ì‚¬ìš©ì ì§ˆë¬¸")
    args = parser.parse_args()

    # ëª¨ë¸ ë¡œë“œ
    vlm_model, text_tokenizer, vis_tokenizer, embedding_model, collections = load_all_models()

    # ì´ë¯¸ì§€ ë¡œë“œ
    try:
        image = Image.open(args.image_path).convert('RGB')
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {args.image_path}")
        return

    # RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    final_answer, reasoning, _, _ = run_rag_pipeline_v2(
        image,
        args.question,
        vlm_model,
        text_tokenizer,
        vis_tokenizer,
        embedding_model,
        collections
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("                ìµœì¢… ê²°ê³¼")
    print("="*50)
    print(f"\n[ì§ˆë¬¸] {args.question}\n")
    print(f"[ìµœì¢… ë‹µë³€]\n{final_answer}\n")
    print(f"[ê·¼ê±°]\n{reasoning}")
    print("="*50)


if __name__ == "__main__":
    main()
