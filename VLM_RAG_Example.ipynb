{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5479dcc",
   "metadata": {},
   "source": [
    "# Ovis VLM with RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "This notebook demonstrates how to add RAG capabilities to the Ovis Vision-Language Model. By integrating a retrieval system, the model can answer questions using information from an external knowledge base, leading to more accurate and detailed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow sentence-transformers pandas openpyxl chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797dc3f2",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Knowledge Base\n",
    "\n",
    "First, we'll create a simple knowledge base. We'll use a few text documents, split them into manageable chunks, and then convert them into vector embeddings using a sentence transformer model. These embeddings will be stored in a FAISS index for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c01f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reading Excel file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aisw/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Creating combined text columns...\n",
      "Step 3: Total documents to embed: 1070\n",
      "Step 4: Loading embedding model...\n",
      "Step 5: Starting embedding (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 34/34 [00:01<00:00, 33.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Embedding finished.\n",
      "Successfully loaded and combined text from Excel file.\n",
      "Document embeddings created successfully.\n",
      "Shape of embeddings: (1070, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the Excel file\n",
    "try:\n",
    "    print('Step 1: Reading Excel file...')\n",
    "    df = pd.read_excel('/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/korean_train_20250101.xlsx')\n",
    "    print('Step 2: Creating combined text columns...')\n",
    "    text_columns = [\n",
    "        '역명(한글)', '역명(영어)', '운영노선', '역 주소(지번주소)', '역 주소(도로명 주소)', '참고사항'\n",
    "    ]\n",
    "    df['임베딩텍스트'] = df[text_columns].astype(str).agg(' / '.join, axis=1)\n",
    "    documents = df['임베딩텍스트'].tolist()\n",
    "    print(f'Step 3: Total documents to embed: {len(documents)}')\n",
    "\n",
    "    # (선택) 데이터 일부만 사용\n",
    "    # documents = documents[:1000]\n",
    "\n",
    "    print('Step 4: Loading embedding model...')\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-m3')\n",
    "    import torch\n",
    "    print('Step 5: Starting embedding (this may take a while)...')\n",
    "    doc_embeddings = embedding_model.encode(documents, show_progress_bar=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Step 6: Embedding finished.')\n",
    "    print(f\"Successfully loaded and combined text from Excel file.\")\n",
    "    print(\"Document embeddings created successfully.\")\n",
    "    print(\"Shape of embeddings:\", doc_embeddings.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: korean_train_20250101.xlsx not found. Using sample data instead.\")\n",
    "    documents = [\n",
    "        \"광장시장은 대한민국 서울특별시 종로구에 위치한 전통 시장이다.\",\n",
    "        \"1905년에 개설되었으며, 대한민국 최초의 상설 시장으로 알려져 있다.\",\n",
    "        \"주요 판매 품목은 한복, 직물, 구제 의류, 그리고 다양한 먹거리이다.\",\n",
    "        \"특히 빈대떡, 마약김밥, 육회 등이 유명하여 많은 관광객들이 찾는다.\"\n",
    "    ]\n",
    "    print('Step 4: Loading embedding model...')\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-m3')\n",
    "    import torch\n",
    "    print('Step 5: Starting embedding (sample data)...')\n",
    "    doc_embeddings = embedding_model.encode(documents, show_progress_bar=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Step 6: Embedding finished.')\n",
    "    print(\"Document embeddings created successfully.\")\n",
    "    print(\"Shape of embeddings:\", doc_embeddings.shape)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}. Using sample data.\")\n",
    "    documents = [\n",
    "        \"광장시장은 대한민국 서울특별시 종로구에 위치한 전통 시장이다.\",\n",
    "        \"1905년에 개설되었으며, 대한민국 최초의 상설 시장으로 알려져 있다.\",\n",
    "        \"주요 판매 품목은 한복, 직물, 구제 의류, 그리고 다양한 먹거리이다.\",\n",
    "        \"특히 빈대떡, 마약김밥, 육회 등이 유명하여 많은 관광객들이 찾는다.\"\n",
    "    ]\n",
    "    print('Step 4: Loading embedding model...')\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-m3')\n",
    "    import torch\n",
    "    print('Step 5: Starting embedding (sample data)...')\n",
    "    doc_embeddings = embedding_model.encode(documents, show_progress_bar=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Step 6: Embedding finished.')\n",
    "    print(\"Document embeddings created successfully.\")\n",
    "    print(\"Shape of embeddings:\", doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6ca46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB collection 'korean_knowledge_base' created/updated with 1070 documents.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client (in-memory)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create a new collection or get an existing one\n",
    "collection_name = \"korean_knowledge_base\"\n",
    "collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "# Generate IDs for each document\n",
    "doc_ids = [str(i) for i in range(len(documents))]\n",
    "\n",
    "# Add documents to the collection\n",
    "collection.add(\n",
    "    embeddings=doc_embeddings.tolist(),\n",
    "    documents=documents,\n",
    "    ids=doc_ids\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB collection '{collection_name}' created/updated with {collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79580888",
   "metadata": {},
   "source": [
    "## Step 2: Implement Retriever\n",
    "\n",
    "Now, we'll create a retriever function. This function will take a user's query, embed it using the same sentence transformer model, and then search the FAISS index to find the most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0e52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 평촌역의 위도와 경도를 알려주세요\n",
      "Retrieved documents:\n",
      "- 평촌역 / Pyeongchon / 4호선 / nan / 경기도 안양시 동안구 부림로 지하 123 / nan\n",
      "- 강촌역 / Gangchon / 경춘선 / 강원도 춘천시 남산면 방곡리 409 / 강원도 춘천시 남산면 강촌로 150 / nan\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query, k=2):\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    # Return the retrieved documents\n",
    "    return results['documents'][0]\n",
    "\n",
    "# Test the retriever with a Korean query\n",
    "test_query = \"평촌역의 위도와 경도를 알려주세요\"\n",
    "retrieved = retrieve_documents(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"Retrieved documents:\")\n",
    "for doc in retrieved:\n",
    "    print(f\"- {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb36e8e",
   "metadata": {},
   "source": [
    "## Step 3: Load Ovis VLM Model\n",
    "\n",
    "Next, we load the Ovis VLM model and its tokenizers. This code is adapted from your `main.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aa5ec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/AIDC-AI/Ovis2-8B:\n",
      "- configuration_ovis.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/AIDC-AI/Ovis2-8B:\n",
      "- modeling_ovis.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Error while downloading from https://cdn-lfs-us-1.hf.co/repos/1f/b4/1fb40860dac4e815d5d9936016bd3b1801c54c24cef5aed8132a9628677579d9/480011cf1313ddd3be5f9a755a199685520b213053c772ea79d60d1ae1375e3c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00004.safetensors%3B+filename%3D%22model-00001-of-00004.safetensors%22%3B&Expires=1753769174&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mzc2OTE3NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzFmL2I0LzFmYjQwODYwZGFjNGU4MTVkNWQ5OTM2MDE2YmQzYjE4MDFjNTRjMjRjZWY1YWVkODEzMmE5NjI4Njc3NTc5ZDkvNDgwMDExY2YxMzEzZGRkM2JlNWY5YTc1NWExOTk2ODU1MjBiMjEzMDUzYzc3MmVhNzlkNjBkMWFlMTM3NWUzYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=aFzln0oEuszeuVsBqAJjX5mIx49WtPwE9QWIP7xgSmVrDG-O2L2ZwjrYqRApkiSD2%7Elzln%7EzZcZmVxU%7E0PxQDj%7EGHcbWfV3-CToOokErSFcsNsfPRJRnyqyFGRwPIiw-OE3vBmPlmkezS0NddtxFZqvt7VAcNtxHs1t2eH7SA5Tt6c-p5XDo9KokQXYYcLRyQtbmecwaXcS5tO-xkI7VM9eWTkxO8272cMTfnL49k9-bgOY8XhZNCSH%7Ecnh6kiU%7ENW8rrcY50UDQaln6c6ELfNnpbqjuVQvNzPo98Ljz-H%7ElMcO04YtdSAAjZiVDR2loRHi6nf61JAaAKgT6wXKZ5Q__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Downloading shards: 100%|██████████| 4/4 [44:34<00:00, 668.59s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ovis VLM model and tokenizers loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "model_path = \"AIDC-AI/Ovis2-8B\"\n",
    "\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"./hf_cache\",\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()\n",
    "\n",
    "print(\"Ovis VLM model and tokenizers loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f3427",
   "metadata": {},
   "source": [
    "## Step 4: RAG-Enhanced Inference\n",
    "\n",
    "Finally, we'll combine everything. We'll take an image and a question, use our retriever to find relevant information, construct a new prompt with this context, and then feed it to the Ovis model to get a RAG-enhanced answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62a16060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 이미지 설명 ---\n",
      "이 이미지는 서울 지하철 4호선의 평촌역을 나타내는 표지판입니다. 표지판은 파란색 테두리와 흰색 배경으로 구성되어 있으며, 왼쪽에는 파란색 원 안에 \"441\"이라는 숫자가 표시되어 있습니다. 오른쪽에는 \"평촌 (한림대성심병원)\"이라는 한글과 \"Pyeongchon\"이라는 영문, 그리고 \"坪村\"이라는 한자 표기가 있습니다. 표지판은 벽에 부착되어 있으며, 벽은 베이지색과 분홍색 타일로 구성되어 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAG 기반 답변 ---\n",
      "결과:\n",
      "평촌역은 4호선에 위치한 역으로, 서울역과 환승이 가능합니다.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 이미지로부터 Ovis가 1차 설명 생성\n",
    "import os\n",
    "image_path = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/Pyeongchon_station.jpg'  # 실제 이미지 경로로 변경\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Warning: Image not found at {image_path}. Please upload an image.\")\n",
    "    images = [Image.new('RGB', (512, 512), color='blue')]\n",
    "else:\n",
    "    images = [Image.open(image_path)]\n",
    "\n",
    "# 1차 프롬프트: 이미지만 보고 설명 생성\n",
    "image_only_prompt = \"이 이미지를 보고 간단히 설명해 주세요. <image>\"\n",
    "max_partition = 9\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(image_only_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=256, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    image_description = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- 이미지 설명 ---\")\n",
    "print(image_description)\n",
    "\n",
    "# Step 2: 이미지 설명을 기반으로 벡터DB에서 관련 정보 검색\n",
    "k = 3\n",
    "query_embedding = embedding_model.encode([image_description], show_progress_bar=False, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = collection.query(query_embeddings=query_embedding.tolist(), n_results=k)\n",
    "retrieved_context = results['documents'][0]\n",
    "context_str = \"\\n\".join(retrieved_context)\n",
    "\n",
    "# Step 3: 최종 RAG 프롬프트 생성 및 응답\n",
    "user_question = \"이 역에서 환승이 가능한가요?\"  # 실제 질문으로 교체 가능\n",
    "\n",
    "# --- 프롬프트 강화: 문맥이 없으면 답변하지 않기 ---\n",
    "rag_prompt = f\"\"\"\n",
    "아래 [문맥]에 주어진 정보만을 근거로 [질문]에 답변하세요.\n",
    "- 반드시 [문맥]의 내용을 참고하여 답변하세요.\n",
    "- [문맥]에 답이 없으면, 'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'라고 답하세요.\n",
    "- [문맥]에 없는 내용을 상상하거나 지어내지 마세요.\n",
    "\n",
    "[문맥]\n",
    "{context_str if context_str.strip() else '없음'}\n",
    "\n",
    "[질문]\n",
    "{user_question}\n",
    "<image>\n",
    "\"\"\"\n",
    "\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(rag_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=1024, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- RAG 기반 답변 ---\")\n",
    "print(f'결과:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aef0a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'언주 / Eonju / 9호선 / 서울특별시 강남구 논현동 279-165 / 서울특별시 강남구 봉은사로 201 / nan\\n금남로5가 / Geumnamno 5(o)-ga  / 1호선 / 광주광역시 북구 북동 299 / 광주광역시 북구 금남로 지하 138(북동) / nan\\n수서역 / Suseo / GTX-A / 서울 강남구 수서동 728 / 서울 강남구 광평로 지하270 / nan'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
