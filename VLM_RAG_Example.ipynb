{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5479dcc",
   "metadata": {},
   "source": [
    "# Ovis VLM with RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "This notebook demonstrates how to add RAG capabilities to the Ovis Vision-Language Model. By integrating a retrieval system, the model can answer questions using information from an external knowledge base, leading to more accurate and detailed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Pillow sentence-transformers pandas openpyxl chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b151373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen/QwQ-32B 설치 (transformers, accelerate 등 필요)\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers accelerate\n",
    "# Qwen/QwQ-32B 모델은 transformers에서 자동 다운로드됨 (최초 실행 시 시간 소요)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797dc3f2",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Knowledge Base\n",
    "\n",
    "First, we'll create a simple knowledge base. We'll use a few text documents, split them into manageable chunks, and then convert them into vector embeddings using a sentence transformer model. These embeddings will be stored in a FAISS index for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c01f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aisw/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reading Excel file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aisw/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Creating combined text columns...\n",
      "Step 3: Total documents to embed: 1070\n",
      "Step 4: Loading embedding model...\n",
      "Step 5: Starting embedding (this may take a while)...\n",
      "Step 5: Starting embedding (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 34/34 [00:02<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Embedding finished.\n",
      "Successfully loaded and combined text from Excel file.\n",
      "Document embeddings created successfully.\n",
      "Shape of embeddings: (1070, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the Excel file\n",
    "try:\n",
    "    print('Step 1: Reading Excel file...')\n",
    "    df = pd.read_excel('/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/korean_train_20250101.xlsx')\n",
    "    print('Step 2: Creating combined text columns...')\n",
    "    # 모든 컬럼을 문자열로 합침\n",
    "    all_columns = df.columns.tolist()\n",
    "    df['임베딩텍스트'] = df[all_columns].astype(str).agg(' / '.join, axis=1)\n",
    "    documents = df['임베딩텍스트'].tolist()\n",
    "    print(f'Step 3: Total documents to embed: {len(documents)}')\n",
    "\n",
    "    # (선택) 데이터 일부만 사용\n",
    "    # documents = documents[:1000]\n",
    "\n",
    "    print('Step 4: Loading embedding model...')\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-m3')\n",
    "    import torch\n",
    "    print('Step 5: Starting embedding (this may take a while)...')\n",
    "    doc_embeddings = embedding_model.encode(documents, show_progress_bar=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Step 6: Embedding finished.')\n",
    "    print(f\"Successfully loaded and combined text from Excel file.\")\n",
    "    print(\"Document embeddings created successfully.\")\n",
    "    print(\"Shape of embeddings:\", doc_embeddings.shape)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: korean_train_20250101.xlsx not found. Using sample data instead.\")\n",
    "    documents = [\n",
    "        \"광장시장은 대한민국 서울특별시 종로구에 위치한 전통 시장이다.\",\n",
    "        \"1905년에 개설되었으며, 대한민국 최초의 상설 시장으로 알려져 있다.\",\n",
    "        \"주요 판매 품목은 한복, 직물, 구제 의류, 그리고 다양한 먹거리이다.\",\n",
    "        \"특히 빈대떡, 마약김밥, 육회 등이 유명하여 많은 관광객들이 찾는다.\"\n",
    "    ]\n",
    "    print('Step 4: Loading embedding model...')\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-m3')\n",
    "    import torch\n",
    "    print('Step 5: Starting embedding (sample data)...')\n",
    "    doc_embeddings = embedding_model.encode(documents, show_progress_bar=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Step 6: Embedding finished.')\n",
    "    print(\"Document embeddings created successfully.\")\n",
    "    print(\"Shape of embeddings:\", doc_embeddings.shape)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}. Using sample data.\")\n",
    "    documents = [\n",
    "        \"광장시장은 대한민국 서울특별시 종로구에 위치한 전통 시장이다.\",\n",
    "        \"1905년에 개설되었으며, 대한민국 최초의 상설 시장으로 알려져 있다.\",\n",
    "        \"주요 판매 품목은 한복, 직물, 구제 의류, 그리고 다양한 먹거리이다.\",\n",
    "        \"특히 빈대떡, 마약김밥, 육회 등이 유명하여 많은 관광객들이 찾는다.\"\n",
    "    ]\n",
    "    print('Step 4: Loading embedding model...')\n",
    "    embedding_model = SentenceTransformer('BAAI/bge-m3')\n",
    "    import torch\n",
    "    print('Step 5: Starting embedding (sample data)...')\n",
    "    doc_embeddings = embedding_model.encode(documents, show_progress_bar=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Step 6: Embedding finished.')\n",
    "    print(\"Document embeddings created successfully.\")\n",
    "    print(\"Shape of embeddings:\", doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6ca46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB collection 'korean_knowledge_base' created/updated with 1070 documents. (persisted at ./chroma_db)\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# 프로젝트 내에 chroma_db 폴더에 영구 저장\n",
    "persist_dir = './chroma_db'\n",
    "client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "# Create a new collection or get an existing one\n",
    "collection_name = \"korean_knowledge_base\"\n",
    "collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "# Generate IDs for each document\n",
    "doc_ids = [str(i) for i in range(len(documents))]\n",
    "\n",
    "# Add documents to the collection\n",
    "collection.add(\n",
    "    embeddings=doc_embeddings.tolist(),\n",
    "    documents=documents,\n",
    "    ids=doc_ids\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB collection '{collection_name}' created/updated with {collection.count()} documents. (persisted at {persist_dir})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79580888",
   "metadata": {},
   "source": [
    "## Step 2: Implement Retriever\n",
    "\n",
    "Now, we'll create a retriever function. This function will take a user's query, embed it using the same sentence transformer model, and then search the FAISS index to find the most relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0e52ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 평촌역의 위도와 경도를 알려주세요\n",
      "Retrieved documents:\n",
      "- 코레일 / 4호선 / 도시/광역철도 / 441 / 평촌역 / Pyeongchon / Pyeongchon / ピョンチョン / 坪村 / 坪村 / nan / 불가능 / nan / 없음 / 없음 / 있음 / 있음 / 상대식 / 126.963881 / 37.394346 / nan / 경기도 안양시 동안구 부림로 지하 123 / 031-383-7788 / 19930115 / nan / 1.6 / 1.3 / 20231220 / nan / nan\n",
      "- 코레일 / 경의중앙선 / 도시/광역철도 / P314 / 신촌 / Sinchon / Sinchon / シンチョン / 新村 / 新村 / nan / 불가능 / nan / 없음 / 없음 / 있음 / 있음 / 섬식 / 126.942308 / 37.559768 / 서울특별시 신촌동 74-12 / 서울특별시 서대문구 신촌역로 30 / 02-363-7788 / 2009-07-01 00:00:00 / nan / 3.1 / 2.7 / 20231220 / nan / nan\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query, k=2):\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    # Return the retrieved documents\n",
    "    return results['documents'][0]\n",
    "\n",
    "# Test the retriever with a Korean query\n",
    "test_query = \"평촌역의 위도와 경도를 알려주세요\"\n",
    "retrieved = retrieve_documents(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"Retrieved documents:\")\n",
    "for doc in retrieved:\n",
    "    print(f\"- {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb36e8e",
   "metadata": {},
   "source": [
    "## Step 3: Load Ovis VLM Model\n",
    "\n",
    "Next, we load the Ovis VLM model and its tokenizers. This code is adapted from your `main.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa5ec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ovis VLM model and tokenizers loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "model_path = \"AIDC-AI/Ovis2-8B\"\n",
    "\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"./hf_cache\",\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()\n",
    "\n",
    "print(\"Ovis VLM model and tokenizers loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f3427",
   "metadata": {},
   "source": [
    "## Step 4: RAG-Enhanced Inference\n",
    "\n",
    "Finally, we'll combine everything. We'll take an image and a question, use our retriever to find relevant information, construct a new prompt with this context, and then feed it to the Ovis model to get a RAG-enhanced answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a16060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 이미지로부터 Ovis가 1차 설명 생성\n",
    "import os\n",
    "image_path = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/Pyeongchon_station.jpg'  # 실제 이미지 경로로 변경\n",
    "image_path = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/seoul_station.jpg'  # 실제 이미지 경로로 변경\n",
    "\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Warning: Image not found at {image_path}. Please upload an image.\")\n",
    "    images = [Image.new('RGB', (512, 512), color='blue')]\n",
    "else:\n",
    "    images = [Image.open(image_path)]\n",
    "\n",
    "# 1차 프롬프트: 이미지만 보고 설명 생성\n",
    "image_only_prompt = \"이 이미지를 보고 간단히 설명해 주세요. <image>\"\n",
    "max_partition = 9\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(image_only_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=256, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    image_description = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- 이미지 설명 ---\")\n",
    "print(image_description)\n",
    "\n",
    "# Step 2: 이미지 설명을 기반으로 벡터DB에서 관련 정보 검색\n",
    "k = 3\n",
    "query_embedding = embedding_model.encode([image_description], show_progress_bar=False, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = collection.query(query_embeddings=query_embedding.tolist(), n_results=k)\n",
    "retrieved_context = results['documents'][0]\n",
    "context_str = \"\\n\".join(retrieved_context)\n",
    "\n",
    "# Step 3: 최종 RAG 프롬프트 생성 및 응답\n",
    "user_question = \"이 역에서 어떤 노선으로 환승할 수 있나요?\"   # 실제 질문으로 교체 가능\n",
    "\n",
    "# --- 프롬프트 더욱 엄격하게 보완 ---\n",
    "rag_prompt = f\"\"\"\n",
    "아래 [문맥]에 주어진 정보만을 근거로 [질문]에 답변하세요.\n",
    "- 반드시 [문맥]의 내용을 직접 인용하거나 요약하여 답변하세요.\n",
    "- [문맥]에 답이 없거나 불충분하면, 반드시 아래 예시처럼만 답변하세요:\n",
    "  'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\n",
    "- [문맥]에 없는 내용을 상상하거나 지어내지 마세요. 추가 설명도 하지 마세요.\n",
    "- 예시)\n",
    "  [문맥]이 비어있거나 관련 정보가 없을 때: 'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\n",
    "\n",
    "[문맥]\n",
    "{context_str if context_str.strip() else '없음'}\n",
    "\n",
    "[질문]\n",
    "{user_question}\n",
    "<image>\n",
    "\"\"\"\n",
    "\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(rag_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=1024, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- RAG 기반 답변 ---\")\n",
    "print(f'결과:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 이미지로부터 Ovis가 1차 설명 생성\n",
    "import os\n",
    "image_path = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/Daejeon_station.jpeg'  # 실제 이미지 경로로 변경\n",
    "\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Warning: Image not found at {image_path}. Please upload an image.\")\n",
    "    images = [Image.new('RGB', (512, 512), color='blue')]\n",
    "else:\n",
    "    images = [Image.open(image_path)]\n",
    "\n",
    "# 1차 프롬프트: 이미지만 보고 설명 생성\n",
    "image_only_prompt = \"이 이미지를 보고 간단히 설명해 주세요. <image>\"\n",
    "max_partition = 9\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(image_only_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=256, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    image_description = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- 이미지 설명 ---\")\n",
    "print(image_description)\n",
    "\n",
    "# Step 2: 이미지 설명을 기반으로 벡터DB에서 관련 정보 검색\n",
    "k = 3\n",
    "query_embedding = embedding_model.encode([image_description], show_progress_bar=False, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = collection.query(query_embeddings=query_embedding.tolist(), n_results=k)\n",
    "retrieved_context = results['documents'][0]\n",
    "context_str = \"\\n\".join(retrieved_context)\n",
    "\n",
    "# Step 3: 최종 RAG 프롬프트 생성 및 응답\n",
    "user_question = \"대전역의 역사 전화번호는 어떻게 되나요?\"   # 실제 질문으로 교체 가능\n",
    "\n",
    "# --- 프롬프트 더욱 엄격하게 보완 ---\n",
    "rag_prompt = f\"\"\"\n",
    "아래 [문맥]에 주어진 정보만을 근거로 [질문]에 답변하세요.\n",
    "- 반드시 [문맥]의 내용을 직접 인용하거나 요약하여 답변하세요.\n",
    "- [문맥]에 답이 없거나 불충분하면, 반드시 아래 예시처럼만 답변하세요:\n",
    "  'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\n",
    "- [문맥]에 없는 내용을 상상하거나 지어내지 마세요. 추가 설명도 하지 마세요.\n",
    "- 예시)\n",
    "  [문맥]이 비어있거나 관련 정보가 없을 때: 'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\n",
    "\n",
    "[문맥]\n",
    "{context_str if context_str.strip() else '없음'}\n",
    "\n",
    "[질문]\n",
    "{user_question}\n",
    "<image>\n",
    "\"\"\"\n",
    "\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(rag_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=1024, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- RAG 기반 답변 ---\")\n",
    "print(f'결과:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8146e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 이미지 설명 ---\n",
      "이 이미지는 한국의 지하철 내부를 보여줍니다. 상단에는 시간이 11:52로 표시되어 있으며, \"신내행\"이라는 목적지와 \"U+5G 갤러리 공덕역 오픈\"이라는 문구가 눈에 띕니다. 또한, \"6호선 공덕역 플랫폼 및 환승공간\"이라는 안내문이 있습니다. 오른쪽에는 지하철 내부의 모습이 담긴 홍보 영상이 재생되고 있습니다. 하단에는 \"개인회생·파산면책\"이라는 문구와 함께 \"1811-1890\"이라는 전화번호가 표시되어 있습니다. 이는 개인회생 및 파산면책 상담을 위한 전화번호입니다. 또한, 지하철 노선도와 함께 \"무료 상담\"이라는 문구가 보입니다.\n",
      "--- RAG 기반 답변 ---\n",
      "결과:\n",
      "현재 타고 있는 라인은 5호선입니다. 이는 디지털 표시판에 '공덕' 역이 표시되어 있고, 'U+5G 갤러리 공덕역 오픈'이라는 문구가 보이기 때문입니다. 또한, 지하철 내부의 광고판에 '5%'라는 숫자가 표시되어 있어 5호선을 타고 있다는 것을 추측할 수 있습니다.\n",
      "--- RAG 기반 답변 ---\n",
      "결과:\n",
      "현재 타고 있는 라인은 5호선입니다. 이는 디지털 표시판에 '공덕' 역이 표시되어 있고, 'U+5G 갤러리 공덕역 오픈'이라는 문구가 보이기 때문입니다. 또한, 지하철 내부의 광고판에 '5%'라는 숫자가 표시되어 있어 5호선을 타고 있다는 것을 추측할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 이미지로부터 Ovis가 1차 설명 생성\n",
    "import os\n",
    "image_path = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/test1.jpg'  # 실제 이미지 경로로 변경\n",
    "\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Warning: Image not found at {image_path}. Please upload an image.\")\n",
    "    images = [Image.new('RGB', (512, 512), color='blue')]\n",
    "else:\n",
    "    images = [Image.open(image_path)]\n",
    "\n",
    "# 1차 프롬프트: 이미지만 보고 설명 생성\n",
    "image_only_prompt = \"이 이미지를 보고 간단히 설명해 주세요. <image>\"\n",
    "max_partition = 9\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(image_only_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=256, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    image_description = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- 이미지 설명 ---\")\n",
    "print(image_description)\n",
    "\n",
    "# Step 2: 이미지 설명을 기반으로 벡터DB에서 관련 정보 검색\n",
    "k = 3\n",
    "query_embedding = embedding_model.encode([image_description], show_progress_bar=False, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = collection.query(query_embeddings=query_embedding.tolist(), n_results=k)\n",
    "retrieved_context = results['documents'][0]\n",
    "context_str = \"\\n\".join(retrieved_context)\n",
    "\n",
    "# Step 3: 최종 RAG 프롬프트 생성 및 응답\n",
    "user_question = \"현재 몇호선 라인을 타고있다고 추측할 수 있나요? 그 이유가 뭔가요?\"   # 실제 질문으로 교체 가능\n",
    "\n",
    "# --- 프롬프트 더욱 엄격하게 보완 ---\n",
    "rag_prompt = f\"\"\"\n",
    "당신은 지하철 정보에 특화된 질의응답 전문가입니다.\n",
    "당신은 이미지 분석 후 생성된 문서를 기반으로 정확하고 검증된 정보를 제공하는 지식 기반 응답 전문가입니다.\n",
    "아래 [문맥]에 주어진 정보만을 근거로 [질문]에 답변하세요.\n",
    "- 반드시 [문맥]의 내용을 직접 인용하거나 요약하여 답변하세요.\n",
    "- [문맥]에 답이 없거나 불충분하면, 반드시 아래 예시처럼만 답변하세요:\n",
    "  'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\n",
    "- [문맥]에 없는 내용을 상상하거나 지어내지 마세요. 추가 설명도 하지 마세요.\n",
    "- 예시)\n",
    "  [문맥]이 비어있거나 관련 정보가 없을 때: 'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\n",
    "\n",
    "[문맥]\n",
    "{context_str if context_str.strip() else '없음'}\n",
    "\n",
    "[질문]\n",
    "{user_question}\n",
    "<image>\n",
    "\"\"\"\n",
    "\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(rag_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=1024, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- RAG 기반 답변 ---\")\n",
    "print(f'결과:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a402a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 이미지 설명 ---\n",
      "이 이미지는 한국의 지하철역 내부에서 촬영된 것으로 보입니다. 상단에는 \"영구·지하철 연락전용 화면\"이라는 제목이 적힌 디지털 표지판이 보입니다. 표지판에는 다음 열차의 출발 정보가 표시되어 있습니다.\n",
      "\n",
      "1. **표지판 내용**:\n",
      "   - **출발 순서**: 1번, 2번, 3번\n",
      "   - **열차 번호**: 1호열차\n",
      "   - **행선지**: 덕소, 용문, 덕소\n",
      "   - **출발 시간**: 12:26, 12:41, 12:53\n",
      "   - **상태**: 출발 예정 (출발 시간이 빨간색으로 표시되어 있음)\n",
      "\n",
      "2. **배경**:\n",
      "   - **지하철역 내부**: 천장에 설치된 디지털 표지판과 지하철역 내부의 일반적인 구조가 보입니다.\n",
      "   - **안내 표지판**: 표지판 아래에는 \"행신·일산·문산\"과 \"서울역·신촌·화전\"으로 이동하는 방향을 가리키는 파란색 안내 표지판이 있습니다.\n",
      "\n",
      "이 이미지는 지하철역 내부에서 열차의 출발 정보를 확인할 수 있는 디지털 표지판을 보여주고 있습니다.\n",
      "['서울교통공사 / 4호선 / 도시/광역철도 / 426 / 서울역 / Seoul Station / nan / ソウルヨク / 首尔站 / nan / nan / O / 1호선, 공항철도, 경의중앙선 / 없음 / 있음 / 있음 / nan / 섬식 / 126.972836 / 37.553172 / 서울특별시 용산구 동자동 14-151 서울역 (4호선) / 서울특별시 용산구 한강대로 지하392(동자동) / 02-6110-4261 / 19851018 / nan / 0.8999999999999986 / 0.8999999999999986 / 20241231 / nan / nan', '대구교통공사 / 2호선 / 도시/광역철도 / 0220 / 계명대 / Keimyung Univ. / 없음 / 없음 / 없음 / 啓明大 / 없음 / 불가능 / 없음 / 없음 / 있음 / 있음 / 없음 / 상대식 / 128.49196903 / 35.85146491 / - / 대구광역시 달서구 달구벌대로 지하1140 / 053-588-0990 / 20051018 / - / 1.3 / 1.2 / 20231231 / nan / nan', '서울교통공사 / 2호선 / 도시/광역철도 / 201 / 시청 / City Hall / nan / シチョン / 市厅 / nan / nan / O / 1호선 / 있음 / 있음 / 있음 / nan / 섬식 / 126.975271 / 37.563534 / 서울특별시 중구 서소문동 90-1 시청역(2호선) / 서울특별시 중구 서소문로 지하127(서소문동) / 02-6110-2011 / 19840522 / nan / 1.1000000000000014 / 1.1000000000000014 / 20241231 / nan / nan']\n",
      "--- RAG 기반 답변 ---\n",
      "결과:\n",
      "이 전광판은 서울교통공사의 4호선에 대한 전광판입니다. 이는 [문맥]에서 제공된 정보를 통해 확인할 수 있습니다.\n",
      "--- RAG 기반 답변 ---\n",
      "결과:\n",
      "이 전광판은 서울교통공사의 4호선에 대한 전광판입니다. 이는 [문맥]에서 제공된 정보를 통해 확인할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 이미지로부터 Ovis가 1차 설명 생성\n",
    "import os\n",
    "image_path = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/where.png'  # 실제 이미지 경로로 변경\n",
    "\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Warning: Image not found at {image_path}. Please upload an image.\")\n",
    "    images = [Image.new('RGB', (512, 512), color='blue')]\n",
    "else:\n",
    "    images = [Image.open(image_path)]\n",
    "\n",
    "# 1차 프롬프트: 이미지만 보고 설명 생성\n",
    "image_only_prompt = \"이 이미지를 보고 간단히 설명해 주세요. <image>\"\n",
    "max_partition = 9\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(image_only_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=1024, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    image_description = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- 이미지 설명 ---\")\n",
    "print(image_description)\n",
    "\n",
    "# Step 2: 이미지 설명을 기반으로 벡터DB에서 관련 정보 검색\n",
    "k = 3\n",
    "query_embedding = embedding_model.encode([image_description], show_progress_bar=False, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "results = collection.query(query_embeddings=query_embedding.tolist(), n_results=k)\n",
    "retrieved_context = results['documents'][0]\n",
    "context_str = \"\\n\".join(retrieved_context)\n",
    "print(retrieved_context)\n",
    "\n",
    "# Step 3: 최종 RAG 프롬프트 생성 및 응답\n",
    "user_question = \"지금 보고 있는 전광판은 몇호선에 대한 전광판인가요?\"   # 실제 질문으로 교체 가능\n",
    "\n",
    "# --- 프롬프트 더욱 엄격하게 보완 ---\n",
    "rag_prompt = f\"\"\"\n",
    "당신은 지하철 정보에 특화된 질의응답 전문가입니다.\n",
    "당신은 이미지 분석 후 생성된 문서를 기반으로 정확하고 검증된 정보를 제공하는 지식 기반 응답 전문가입니다.\n",
    "아래 [문맥]에 주어진 정보만을 근거로 [질문]에 답변하세요.\n",
    "이유도 필히 포함하여 답변하세요.\n",
    "- 반드시 [문맥]의 내용을 직접 인용하거나 요약하여 답변하세요.\n",
    "- [문맥]에 답이 없거나 불충분하면, 반드시 아래 예시처럼만 답변하세요:\n",
    "  'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\n",
    "- [문맥]에 없는 내용을 상상하거나 지어내지 마세요. 추가 설명도 하지 마세요.\n",
    "- 예시)\n",
    "  [문맥]이 비어있거나 관련 정보가 없을 때: 'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\n",
    "\n",
    "[문맥]\n",
    "{context_str if context_str.strip() else '없음'}\n",
    "\n",
    "[질문]\n",
    "{user_question}\n",
    "<image>\n",
    "\"\"\"\n",
    "\n",
    "prompt, input_ids, pixel_values = model.preprocess_inputs(rag_prompt, images, max_partition=max_partition)\n",
    "attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "if pixel_values is not None:\n",
    "    pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "pixel_values = [pixel_values]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_kwargs = dict(max_new_tokens=1024, do_sample=False)\n",
    "    output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"--- RAG 기반 답변 ---\")\n",
    "print(f'결과:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d6b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9643ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aisw/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_446985/2983381715.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
      "/tmp/ipykernel_446985/2983381715.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
      "/tmp/ipykernel_446985/2983381715.py:18: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
      "/tmp/ipykernel_446985/2983381715.py:18: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
      "Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]Cancellation requested; stopping current tasks.\n",
      "Downloading shards:  21%|██▏       | 3/14 [03:23<12:25, 67.81s/it]\n",
      "Cancellation requested; stopping current tasks.\n",
      "Downloading shards:  21%|██▏       | 3/14 [03:23<12:25, 67.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:627\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    625\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 627\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m})\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 3. LLM 준비 (Qwen/QwQ-32B로 변경)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m llm_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/QwQ-32B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_bf16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(pipeline\u001b[38;5;241m=\u001b[39mllm_pipe)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 4. Ovis VLM 모델 및 토크나이저 로드 (이미지 설명 생성용)\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:926\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 926\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    937\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:289\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3974\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3971\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3973\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3974\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3985\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3986\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3990\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3991\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3992\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3993\u001b[0m ):\n\u001b[1;32m   3994\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/transformers/utils/hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1161\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1161\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1174\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1710\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[1;32m   1709\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1710\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Project/UST-ETRI-2025/VLM_RAG/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:627\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    625\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 627\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LangChain 기반 이미지→설명→문맥검색→답변 체인 (Qwen/QwQ-32B 활용)\n",
    "import os\n",
    "from PIL import Image\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 1. 임베딩 모델 준비 (sentence-transformers 호환)\n",
    "embedding_model_name = 'BAAI/bge-m3'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# 2. ChromaDB 벡터스토어 연결 (이미 생성된 chroma_db 사용)\n",
    "persist_dir = './chroma_db'\n",
    "vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. LLM 준비 (Qwen/QwQ-32B로 변경)\n",
    "llm_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"Qwen/QwQ-32B\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipe)\n",
    "\n",
    "# 4. Ovis VLM 모델 및 토크나이저 로드 (이미지 설명 생성용)\n",
    "model_path = \"AIDC-AI/Ovis2-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"./hf_cache\",\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()\n",
    "\n",
    "# 5. 이미지→설명→문맥검색→답변 함수\n",
    "def langchain_image_rag(image_path, user_question):\n",
    "    # (1) 이미지 로드 및 Ovis VLM 설명 생성\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Warning: Image not found at {image_path}. Using blank image.\")\n",
    "        images = [Image.new('RGB', (512, 512), color='blue')]\n",
    "    else:\n",
    "        images = [Image.open(image_path)]\n",
    "    image_only_prompt = \"이 이미지를 보고 간단히 설명해 주세요. <image>\"\n",
    "    max_partition = 9\n",
    "    prompt, input_ids, pixel_values = model.preprocess_inputs(image_only_prompt, images, max_partition=max_partition)\n",
    "    attention_mask = torch.ne(input_ids, tokenizer.pad_token_id)\n",
    "    input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "    if pixel_values is not None:\n",
    "        pixel_values = pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)\n",
    "    pixel_values = [pixel_values]\n",
    "    with torch.inference_mode():\n",
    "        gen_kwargs = dict(max_new_tokens=256, do_sample=False)\n",
    "        output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "        image_description = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(\"--- 이미지 설명 ---\")\n",
    "    print(image_description)\n",
    "\n",
    "    # (2) 이미지 설명으로 벡터DB 검색\n",
    "    retrieved_docs = retriever.get_relevant_documents(image_description)\n",
    "    context_str = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # (3) LangChain RAG QA 체인 실행 (프롬프트 엄격화)\n",
    "    prompt_template = (\n",
    "        \"아래 [문맥]에 주어진 정보만을 근거로 [질문]에 답변하세요.\\n\"\n",
    "        \"- 반드시 [문맥]의 내용을 직접 인용하거나 요약하여 답변하세요.\\n\"\n",
    "        \"- [문맥]에 답이 없거나 불충분하면, 반드시 아래 예시처럼만 답변하세요:\\n\"\n",
    "        \"  'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\\n\"\n",
    "        \"- [문맥]에 없는 내용을 상상하거나 지어내지 마세요. 추가 설명도 하지 마세요.\\n\"\n",
    "        \"- 예시) [문맥]이 비어있거나 관련 정보가 없을 때: 'VectorDB(지식베이스)에서 답변을 찾을 수 없습니다.'\\n\\n\"\n",
    "        \"[문맥]\\n{context}\\n\\n[질문]\\n{question}\\n\"\n",
    "    )\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template}\n",
    "    )\n",
    "    # (4) 답변 생성\n",
    "    result = qa_chain({\"query\": user_question, \"context\": context_str})\n",
    "    print(\"--- LangChain 기반 RAG 답변 (Qwen/QwQ-32B) ---\")\n",
    "    print(result[\"result\"])\n",
    "    return result[\"result\"]\n",
    "\n",
    "# 사용 예시\n",
    "image_path = '/home/aisw/Project/UST-ETRI-2025/VLM_RAG/data/where.png'  # 실제 이미지 경로로 변경\n",
    "user_question = \"지금 보고 있는 전광판은 몇호선에 대한 전광판인가요?\"   # 실제 질문으로 교체 가능\n",
    "langchain_image_rag(image_path, user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34f9beb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Qwen/QwQ-32B LLM 준비 (transformers pipeline)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m qwen_pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/QwQ-32B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16 \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_bf16_supported() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      6\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Qwen/QwQ-32B LLM 준비 (transformers pipeline)\n",
    "qwen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"Qwen/QwQ-32B\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=1024,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
